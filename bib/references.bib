
@INPROCEEDINGS{Luo_2020_CVPR,
  author={Luo, Gen and Zhou, Yiyi and Sun, Xiaoshuai and Cao, Liujuan and Wu, Chenglin and Deng, Cheng and Ji, Rongrong},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Multi-Task Collaborative Network for Joint Referring Expression Comprehension and Segmentation}, 
  year={2020},
  volume={},
  number={},
  pages={10031-10040},
  abstract={Referring expression comprehension (REC) and segmentation (RES) are two highly-related tasks, which both aim at identifying the referent according to a natural language expression. In this paper, we propose a novel Multi-task Collaborative Network (MCN) to achieve a joint learning of REC and RES for the first time. In MCN, RES can help REC to achieve better language-vision alignment, while REC can help RES to better locate the referent. In addition, we address a key challenge in this multi-task setup, i.e., the prediction conflict, with two innovative designs namely, Consistency Energy Maximization (CEM) and Adaptive Soft Non-Located Suppression (ASNLS). Specifically, CEM enables REC and RES to focus on similar visual regions by maximizing the consistency energy between two tasks. ASNLS supresses the response of unrelated regions in RES based on the prediction of REC. To validate our model, we conduct extensive experiments on three benchmark datasets of REC and RES, i.e., RefCOCO, RefCOCO+ and RefCOCOg. The experimental results report the significant performance gains of MCN over all existing methods, i.e., up to +7.13% for REC and +11.50% for RES over SOTA, which well confirm the validity of our model for joint REC and RES learning.},
  keywords={type:Unified Frameworks, Task analysis;Feature extraction;Frequency modulation;Visualization;Collaborative work;Tensile stress;Linguistics},
  doi={10.1109/CVPR42600.2020.01005},
  ISSN={2575-7075},
  month={June},}
@misc{lin2022learningobjectlanguagealignmentsopenvocabulary,
      title={Learning Object-Language Alignments for Open-Vocabulary Object Detection}, 
      abstract = {Existing object detection methods are bounded in a fixed-set vocabulary by costly labeled data. When dealing with novel categories, the model has to be retrained with more bounding box annotations. Natural language supervision is an attractive alternative for its annotation-free attributes and broader object concepts. However, learning open-vocabulary object detection from language is challenging since image-text pairs do not contain fine-grained object-language alignments. Previous solutions rely on either expensive grounding annotations or distilling classification-oriented vision models. In this paper, we propose a novel open-vocabulary object detection framework directly learning from image-text pair data. We formulate object-language alignment as a set matching problem between a set of image region features and a set of word embeddings. It enables us to train an open-vocabulary object detector on image-text pairs in a much simple and effective way. Extensive experiments on two benchmark datasets, COCO and LVIS, demonstrate our superior performance over the competing approaches on novel categories, e.g. achieving 32.0% mAP on COCO and 21.7% mask mAP on LVIS.},
      author={Chuang Lin and Peize Sun and Yi Jiang and Ping Luo and Lizhen Qu and Gholamreza Haffari and Zehuan Yuan and Jianfei Cai},
      year={2022},
      keywords = {type:Unified Frameworks,Open-Vocabulary Object Detection, Image-Text Pairs, Region-Word Alignment, Bipartite Matching, VLDet, Zero-Shot Detection, CLIP Embeddings, Hungarian Algorithm, COCO, LVIS, CC3M, Generalization
},
      eprint={2211.14843},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2211.14843}, 
}
@misc{shen2023distilledfeaturefieldsenable,
      title={Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation}, 
      abstract = {Self-supervised and language-supervised image models contain rich knowledge of the world that is important for generalization. Many robotic tasks, however, require a detailed understanding of 3D geometry, which is often lacking in 2D image features. This work bridges this 2D-to-3D gap for robotic manipulation by leveraging distilled feature fields to combine accurate 3D geometry with rich semantics from 2D foundation models. We present a few-shot learning method for 6-DOF grasping and placing that harnesses these strong spatial and semantic priors to achieve in-the-wild generalization to unseen objects. Using features distilled from a vision-language model, CLIP, we present a way to designate novel objects for manipulation via free-text natural language, and demonstrate its ability to generalize to unseen expressions and novel categories of objects.},
      author={William Shen and Ge Yang and Alan Yu and Jansen Wong and Leslie Pack Kaelbling and Phillip Isola},
      year={2023},
      keywords = {type:Domain-Specific Applications,Few-Shot Manipulation, Language-Guided Manipulation, Distilled Feature Fields (DFF), 3D Feature Fields, CLIP, DINO ViT, Neural Radiance Fields (NeRF), Vision-Language Models, 6-DOF Grasping, Open-Ended Generalization, MaskCLIP, Feature Distillation, Semantic Understanding, Robotics, Scene Reconstruction
},
      eprint={2308.07931},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2308.07931}, 
}
@misc{rao2022densecliplanguageguideddenseprediction,
      title={DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting}, 
      author={Yongming Rao and Wenliang Zhao and Guangyi Chen and Yansong Tang and Zheng Zhu and Guan Huang and Jie Zhou and Jiwen Lu},
      abstract = {Recent progress has shown that large-scale pre-training using contrastive image-text pairs can be a promising alternative for high-quality visual representation learning from natural language supervision. Benefiting from a broader source of supervision, this new paradigm exhibits impressive transferability to downstream classification tasks and datasets. However, the problem of transferring the knowledge learned from image-text pairs to more complex dense prediction tasks has barely been visited. In this work, we present a new framework for dense prediction by implicitly and explicitly leveraging the pre-trained knowledge from CLIP. Specifically, we convert the original image-text matching problem in CLIP to a pixel-text matching problem and use the pixel-text score maps to guide the learning of dense prediction models. By further using the contextual information from the image to prompt the language model, we are able to facilitate our model to better exploit the pre-trained knowledge. Our method is model-agnostic, which can be applied to arbitrary dense prediction systems and various pre-trained visual backbones including both CLIP models and ImageNet pre-trained models. Extensive experiments demonstrate the superior performance of our methods on semantic segmentation, object detection, and instance segmentation tasks.},
      keywords = {type:Efficiency-Oriented Models, DenseCLIP, CLIP, Vision-Language Pre-training, Pixel-Text Matching, Context-Aware Prompting, Language-Guided Fine-Tuning, Semantic Segmentation, Object Detection, Instance Segmentation, Transformer, Prompt Engineering, Feature Fusion, Model-Agnostic Framework, ADE20K, COCO},
      year={2022},
      eprint={2112.01518},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.01518}, 
}
@ARTICLE{10409216,
  author={Chen, Keyan and Liu, Chenyang and Chen, Hao and Zhang, Haotian and Li, Wenyuan and Zou, Zhengxia and Shi, Zhenwei},
  journal={IEEE Transactions on Geoscience and Remote Sensing}, 
  title={RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation Based on Visual Foundation Model}, 
  year={2024},
  volume={62},
  number={},
  pages={1-17},
  abstract={Leveraging the extensive training data from SA-1B, the segment anything model (SAM) demonstrates remarkable generalization and zero-shot capabilities. However, as a category-agnostic instance segmentation method, SAM heavily relies on prior manual guidance, including points, boxes, and coarse-grained masks. Furthermore, its performance in remote sensing image segmentation tasks remains largely unexplored and unproven. In this article, we aim to develop an automated instance segmentation approach for remote sensing images based on the foundational SAM model and incorporating semantic category information. Drawing inspiration from prompt learning, we propose a method to learn the generation of appropriate prompts for SAM. This enables SAM to produce semantically discernible segmentation results for remote sensing images, a concept that we have termed RSPrompter. We also propose several ongoing derivatives for instance segmentation tasks, drawing on recent advancements within the SAM community, and compare their performance with RSPrompter. Extensive experimental results, derived from the WHU building dataset, the NWPU VHR-10 dataset, and the SAR Ship Detection Dataset (SSDD) dataset, validate the effectiveness of our proposed method. The code for our method is publicly available at https://kychen.me/RSPrompter.},
  keywords={type:Domain-Specific Applications,Instance segmentation;Image segmentation;Remote sensing;Task analysis;Sensors;Decoding;Detectors;Foundation model;instance segmentation;prompt learning;remote sensing images;segment anything model (SAM)},
  doi={10.1109/TGRS.2024.3356074},
  ISSN={1558-0644},
  month={},}
@ARTICLE{10445007,
  author={Zhang, Jingyi and Huang, Jiaxing and Jin, Sheng and Lu, Shijian},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Vision-Language Models for Vision Tasks: A Survey}, 
  year={2024},
  volume={46},
  number={8},
  pages={5625-5644},
  abstract={Most visual recognition studies rely heavily on crowd-labelled data in deep neural networks (DNNs) training, and they usually train a DNN for each single visual recognition task, leading to a laborious and time-consuming visual recognition paradigm. To address the two challenges, Vision-Language Models (VLMs) have been intensively investigated recently, which learns rich vision-language correlation from web-scale image-text pairs that are almost infinitely available on the Internet and enables zero-shot predictions on various visual recognition tasks with a single VLM. This paper provides a systematic review of visual language models for various visual recognition tasks, including: (1) the background that introduces the development of visual recognition paradigms; (2) the foundations of VLM that summarize the widely-adopted network architectures, pre-training objectives, and downstream tasks; (3) the widely-adopted datasets in VLM pre-training and evaluations; (4) the review and categorization of existing VLM pre-training methods, VLM transfer learning methods, and VLM knowledge distillation methods; (5) the benchmarking, analysis and discussion of the reviewed methods; (6) several research challenges and potential research directions that could be pursued in the future VLM studies for visual recognition.},
  keywords={type:Foundation Models,Task analysis;Visualization;Training;Deep learning;Surveys;Data models;Predictive models;Big Data;big model;deep learning;deep neural network;knowledge distillation;object detection;pre-training;semantic segmentation;transfer learning;vision-language model;visual recognition;image classification},
  doi={10.1109/TPAMI.2024.3369699},
  ISSN={1939-3539},
  month={Aug},}
@article{SAPKOTA202484,
title = {Comparing YOLOv8 and Mask R-CNN for instance segmentation in complex orchard environments},
journal = {Artificial Intelligence in Agriculture},
volume = {13},
pages = {84-99},
year = {2024},
issn = {2589-7217},
doi = {https://doi.org/10.1016/j.aiia.2024.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S258972172400028X},
author = {Ranjan Sapkota and Dawood Ahmed and Manoj Karkee},
keywords = {type:Domain-Specific Applications,YOLOv8, Mask R-CNN, Deep learning, Machine learning, Automation, Robotics, Artificial intelligence, Machine vision},
abstract = {Instance segmentation, an important image processing operation for automation in agriculture, is used to precisely delineate individual objects of interest within images, which provides foundational information for various automated or robotic tasks such as selective harvesting and precision pruning. This study compares the one-stage YOLOv8 and the two-stage Mask R-CNN machine learning models for instance segmentation under varying orchard conditions across two datasets. Dataset 1, collected in dormant season, includes images of dormant apple trees, which were used to train multi-object segmentation models delineating tree branches and trunks. Dataset 2, collected in the early growing season, includes images of apple tree canopies with green foliage and immature (green) apples (also called fruitlet), which were used to train single-object segmentation models delineating only immature green apples. The results showed that YOLOv8 performed better than Mask R-CNN, achieving good precision and near-perfect recall across both datasets at a confidence threshold of 0.5. Specifically, for Dataset 1, YOLOv8 achieved a precision of 0.90 and a recall of 0.95 for all classes. In comparison, Mask R-CNN demonstrated a precision of 0.81 and a recall of 0.81 for the same dataset. With Dataset 2, YOLOv8 achieved a precision of 0.93 and a recall of 0.97. Mask R-CNN, in this single-class scenario, achieved a precision of 0.85 and a recall of 0.88. Additionally, the inference times for YOLOv8 were 10.9 ms for multi-class segmentation (Dataset 1) and 7.8 ms for single-class segmentation (Dataset 2), compared to 15.6 ms and 12.8 ms achieved by Mask R-CNN's, respectively. These findings show YOLOv8's superior accuracy and efficiency in machine learning applications compared to two-stage models, specifically Mask-R-CNN, which suggests its suitability in developing smart and automated orchard operations, particularly when real-time applications are necessary in such cases as robotic harvesting and robotic immature green fruit thinning.}
}
@misc{li2022maskdinounifiedtransformerbased,
      title={Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation}, 
      author={Feng Li and Hao Zhang and Huaizhe xu and Shilong Liu and Lei Zhang and Lionel M. Ni and Heung-Yeung Shum},
      year={2022},
      abstract = {In this paper we present Mask DINO, a unified object detection and segmentation framework. Mask DINO extends DINO (DETR with Improved Denoising Anchor Boxes) by adding a mask prediction branch which supports all image segmentation tasks (instance, panoptic, and semantic). It makes use of the query embeddings from DINO to dot-product a high-resolution pixel embedding map to predict a set of binary masks. Some key components in DINO are extended for segmentation through a shared architecture and training process. Mask DINO is simple, efficient, and scalable, and it can benefit from joint large-scale detection and segmentation datasets. Our experiments show that Mask DINO significantly outperforms all existing specialized segmentation methods, both on a ResNet-50 backbone and a pre-trained model with SwinL backbone. Notably, Mask DINO establishes the best results to date on instance segmentation (54.5 AP on COCO), panoptic segmentation (59.4 PQ on COCO), and semantic segmentation (60.8 mIoU on ADE20K) among models under one billion parameters.},
      keywords = {type:Unified Frameworks, Mask DINO, Unified Framework, Object Detection, Instance Segmentation, Panoptic Segmentation, Semantic Segmentation, Transformer, DETR, DINO, Query Selection, Denoising Training, Hybrid Matching, Mask Prediction, Multi-task Learning, COCO, ADE20K, Cityscapes
},
      eprint={2206.02777},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2206.02777}, 
}
@misc{zhao2023fastsegment,
      title={Fast Segment Anything}, 
      author={Xu Zhao and Wenchao Ding and Yongqi An and Yinglong Du and Tao Yu and Min Li and Ming Tang and Jinqiao Wang},
      year={2023},
      abstract = {The recently proposed segment anything model (SAM) has made a significant influence in many computer vision tasks. It is becoming a foundation step for many high-level tasks, like image segmentation, image caption, and image editing. However, its huge computation costs prevent it from wider applications in industry scenarios. The computation mainly comes from the Transformer architecture at high-resolution inputs. In this paper, we propose a speed-up alternative method for this fundamental task with comparable performance. By reformulating the task as segments-generation and prompting, we find that a regular CNN detector with an instance segmentation branch can also accomplish this task well. Specifically, we convert this task to the well-studied instance segmentation task and directly train the existing instance segmentation method using only 1/50 of the SA-1B dataset published by SAM authors. With our method, we achieve a comparable performance with the SAM method at 50 times higher run-time speed. We give sufficient experimental results to demonstrate its effectiveness.},
      eprint={2306.12156},
      keywords = {type:Efficiency-Oriented Models, FastSAM, Segment Anything, Real-time Segmentation, YOLOv8-seg, CNN-based Segmentation, Prompt-guided Selection, Instance Segmentation, Box Prompt, Point Prompt, Text Prompt, SA-1B Dataset, CLIP, Edge Detection, Object Proposal, Zero-shot Transfer, Model Compression, Industrial Applications},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2306.12156}, 
}
@inproceedings{deng2025segment,
  title={Segment anything model (sam) for digital pathology: Assess zero-shot segmentation on whole slide imaging},
  author={Deng, Ruining and Cui, Can and Liu, Quan and Yao, Tianyuan and Remedios, Lucas W and Bao, Shunxing and Landman, Bennett A and Wheless, Lee E and Coburn, Lori A and Wilson, Keith T and others},
  booktitle={IS\&T International Symposium on Electronic Imaging},
  volume={37},
  keywords = {type:Domain-Specific Applications,Segment Anything Model, SAM, Zero-shot Segmentation, Digital Pathology, Whole Slide Imaging, Tumor Segmentation, Tissue Segmentation, Nuclei Segmentation, Prompt-based Segmentation, Image Resolution, Multi-scale Analysis, Prompt Selection, Model Fine-tuning, SA-1B, Foundation Model, Dense Instance Segmentation, Clinical Application},
  abstract = {The segment anything model (SAM) was released as a foundation model for image segmentation. The promptable segmentation model was trained by over 1 billion masks on 11M licensed and privacy-respecting images. The model supports zero-shot image segmentation with various segmentation prompts (e.g., points, boxes, masks). It makes the SAM attractive for medical image analysis, especially for digital pathology where the training data are rare. In this study, we evaluate the zero-shot segmentation performance of SAM model on representative segmentation tasks on whole slide imaging (WSI), including (1) tumor segmentation, (2) non-tumor tissue segmentation, (3) cell nuclei segmentation.},
  pages={COIMG--132},
  year={2025},
  url={https://library.imaging.org/ei/articles/37/14/COIMG-132}
}
